from random import choice, randint, random


def get_random(start, end):
	return random()*randint(start, end)


# dimensions = 3
# data_size = 100
#
# real_weights = [get_random(1, 10) for w in range(dimensions)]
# print(real_weights)
# data = []
# for i in range(data_size):
# 	obj = [0] * dimensions
# 	for n in range(dimensions):
# 		obj[n] = get_random(1, 100) * real_weights[n]
#
# 	data.append(obj + [sum(obj)])
# print(data)

data = [[21, 20, 6], [30, 20, 8], [25, 19, 8]]
# data = [[1, 2, 3], [3, 7, 10], [1, 9, 10]]
# data = [[1, 1, 2], [3, 3, 6], [5, 5, 10]]


dimensions = len(data[0])
l = len(data)
h = 0.1  # learning rate
lb = 1  # learning memory rate


weights = [random()] * (dimensions-1)
gradient = [0.1] * (dimensions-1)
epochs = 1000


def q_(weights):
	q = 0
	for i in range(l):
		q += (sum([data[i][n] * weights[n] for n in range(dimensions-1)]) - data[i][-1]) ** 2
	q /= l
	return q


q = q_(weights)


def loss_(x, y, w):
	return (x * w - y) ** 2


# if gradient is negative -> increase w
# else -> decrease w
for _ in range(epochs):
	x1 = choice(data)
	gradient = []
	for n in range(dimensions-1):
		g = (loss_(x1[n], x1[-1], weights[n]-0.01) - loss_(x1[n], x1[-1], weights[n])) / 0.01
		gradient.append(g)
	print(gradient, x1)

	for g in range(len(gradient)):
		if gradient[g] > 0:
			weights[g] += h
		else:
			weights[g] -= h

print("MSE:", q_(weights))
print("Weights:", weights)
input = [40, 10]
print(sum([input[i]*weights[i] for i in range(dimensions-1)]))
